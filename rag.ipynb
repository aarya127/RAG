{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75393fe5",
   "metadata": {},
   "source": [
    "# This Note showcases the RAG usecase\n",
    "- Uses Langchain\n",
    "- FAISS vector Store\n",
    "- Hugging Face Embedings\n",
    "- Demonstrates, how to split the documents into multiple chuncks\n",
    "- Demonstrates, how to query the embedings from the vector store\n",
    "- Demostrates, calling BAM models for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dac4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "#BAM\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schemas import ModelType, GenerateParams\n",
    "from genai.model import Credentials\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b77859-2bcc-4a46-85ae-91282a513f4d",
   "metadata": {},
   "source": [
    "### Pass Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd1a05-aa1c-47c2-8165-e114f010dbd4",
   "metadata": {},
   "source": [
    "Create a file named .env in the same directory and include the following:\n",
    "\n",
    "```\n",
    "GENAI_KEY=YOUR_GENAI_API_KEY\n",
    "GENAI_API=https://workbench-api.res.ibm.com/v1/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5f068-b9c0-4b1d-aaa5-284bb8d7a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_endpoint = os.getenv(\"GENAI_API\", None)\n",
    "\n",
    "# creds object\n",
    "creds = Credentials(api_key=api_key, api_endpoint=api_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ff2c2",
   "metadata": {},
   "source": [
    "## Global settings\n",
    "\n",
    "- chunksize: size of chunks documents need to be splited\n",
    "- chunk_overlap: overlap of the chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 2000\n",
    "chunk_overlap = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67148a",
   "metadata": {},
   "source": [
    "## Loading the pdf, file using the PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecc07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Over-the-Range Microwave with Sensor Cooking.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67ff03",
   "metadata": {},
   "source": [
    "## Total number of documents (pages) in the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#every page in pdf is counted as unique document\n",
    "print (f'You have {len(data)} document(s) in your data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a383ac",
   "metadata": {},
   "source": [
    "## Printing the first page of the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75284a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'There are {len(data[0].page_content)} characters in first page')\n",
    "print(f\"content of first page\\n : {data[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559449c3",
   "metadata": {},
   "source": [
    "## Spliting the documents into multiple chuncks on the chunk size mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= chunk_size, chunk_overlap=chunk_overlap)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2672de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have total documents after split: {len(docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f6263",
   "metadata": {},
   "source": [
    "## Loading Hugging Face Emedings\n",
    "- When you run the below cell for the first time, it does take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"hkunlp/instructor-large\",\n",
    "            model_kwargs={\"device\": \"cpu\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5cedd",
   "metadata": {},
   "source": [
    "## Vector Store- FAISS\n",
    "- We have our documents and embedding ready.\n",
    "- Here we are storing our embeddings and docs in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html?highlight=faiss#faiss\n",
    "# this will take a few minutes to run\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d887240-d505-45b0-904b-8831d8420642",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"db.pkl\", \"wb\") as f:\n",
    "    pickle.dump(db, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec574bd-6402-4635-be3e-8d10c1a64316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the database from disk. If the database is saved, you can load it directly and don't have to regenerate it each time you run the notebook.\n",
    "with open(\"db.pkl\", \"rb\") as f:\n",
    "    db = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436e824",
   "metadata": {},
   "source": [
    "## Lets test our embeddings\n",
    "- We are passing the query, and looking for the closest 3 embedings.\n",
    "- printing out the closest 3 embedings for the query from the documents or pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aaf383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How to cook eggs\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n",
    "print(\"----\")\n",
    "print(docs[1].page_content)\n",
    "print(\"----\")\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb9969",
   "metadata": {},
   "source": [
    "## Creating LLM model\n",
    "- Here we are using LangChainInterface to create out BAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec784c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llm = LangChainInterface(\n",
    "        model=ModelType.FLAN_T5_11B,\n",
    "        credentials=creds,\n",
    "        params=GenerateParams(\n",
    "            decoding_method=\"greedy\",\n",
    "            max_new_tokens=300,\n",
    "            min_new_tokens=15,\n",
    "            repetition_penalty=2,\n",
    "        ).dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7c54c",
   "metadata": {},
   "source": [
    "## Loading lang chain qa\n",
    "- creating a chain to get QA from our BAM modles.\n",
    "- Here we are passing chain_type as stuff, which means we are passing all the embeddings fromt the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(model_llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d486045",
   "metadata": {},
   "source": [
    "## Let' get the embedings for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71062776",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to cook eggs\"\n",
    "doc = db.similarity_search(query, k=3)\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83256a",
   "metadata": {},
   "source": [
    "## Finally it's time for us to call our BAM model\n",
    "- here we are passing all embedding and the query to the BAM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(input_documents=doc, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88884dd6",
   "metadata": {},
   "source": [
    "## End of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a66bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
